{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from chainer import *\n",
    "from itertools import zip_longest\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import datetime\n",
    "from gensim.models import word2vec\n",
    "import random\n",
    "import os\n",
    "\n",
    "class Configuration:\n",
    "    def __init__(self, mode, path):\n",
    "        self.mode = mode\n",
    "        self.path = path\n",
    "        with open(path, \"r\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line != \"\":\n",
    "                    exec(\"self.{}\".format(line))\n",
    "        try:\n",
    "            if self.mode not in ['train', 'test', 'dev']:\n",
    "                raise ValueError('you must set mode = \\'train\\' or \\'test\\' or \\'dev\\'')\n",
    "            if self.use_gpu not in [True, False]:\n",
    "                raise ValueError('you must set use_gpu = True or False')\n",
    "            if self.gpu_device < 0:\n",
    "                raise ValueError('you must set gpu_device >= 0')\n",
    "            if self.source_vocabulary_size < 1:\n",
    "                raise ValueError('you must set source_vocabulary_size >= 1')\n",
    "            if self.target_vocabulary_size < 1:\n",
    "                raise ValueError('you must set target_vocabulary_size >= 1')\n",
    "            if self.embed_size < 1:\n",
    "                raise ValueError('you must set embed_size >= 1')\n",
    "            if self.hidden_size < 1:\n",
    "                raise ValueError('you must set hidden_size >= 1')\n",
    "            if self.epoch < 1:\n",
    "                raise ValueError('you must set epoch >= 1')\n",
    "            if self.use_dropout not in [True, False]:\n",
    "                raise ValueError('you must set use_dropout = True or False')\n",
    "            if self.batch_size < 1:\n",
    "                raise ValueError('you must set batch_size >= 1')\n",
    "            if self.pooling < 1:\n",
    "                raise ValueError('you must set pooling >= 1')\n",
    "            if self.generation_limit < 1:\n",
    "                raise ValueError('you must set generation_limit >= 1')\n",
    "            if self.use_beamsearch not in [True, False]:\n",
    "                raise ValueError('you must set use_beamsearch = True or False')\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            sys.exit()\n",
    "\n",
    "        if self.use_gpu:\n",
    "            import cupy\n",
    "            self.library = cupy\n",
    "        else:\n",
    "            self.library = numpy\n",
    "       \n",
    "        if hasattr(self, \"optimizer\"):\n",
    "            self.optimizer = self.set_optimizer(self.optimizer)\n",
    "        \n",
    "        if hasattr(self, \"generator_optimizer\"):\n",
    "            self.generator_optimizer = self.set_optimizer(self.generator_optimizer)\n",
    "        \n",
    "        if hasattr(self, \"discriminator_optimizer\"):\n",
    "            self.discriminator_optimizer = self.set_optimizer(self.discriminator_optimizer)\n",
    "        \n",
    "        if not self.use_dropout:\n",
    "            self.dropout_rate = 0.0\n",
    "\n",
    "        if not self.use_beamsearch:\n",
    "            self.beam_size = 1\n",
    "\n",
    "        if self.mode == \"dev\":\n",
    "            self.use_beamsearch = False\n",
    "            self.use_reconstructor_beamsearch = False\n",
    "\n",
    "    def set_optimizer(self, opt):\n",
    "        if opt == \"AdaGrad\":\n",
    "            opt = optimizers.AdaGrad(lr = self.learning_rate)\n",
    "        elif opt == \"AdaDelta\":\n",
    "            opt = optimizers.AdaDelta()\n",
    "        elif opt == \"Adam\":\n",
    "            opt = optimizers.Adam()\n",
    "        elif opt == \"SGD\":\n",
    "            opt = optimizers.SGD(lr = self.learning_rate)\n",
    "        return opt\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "    def make(path, vocabulary_size):\n",
    "        self = Vocabulary()\n",
    "        self.word2id = defaultdict(lambda: 0)\n",
    "        self.id2word = dict()\n",
    "        with open(path, \"r\") as f:\n",
    "            word_frequency = defaultdict(lambda: 0)\n",
    "            for words in f:\n",
    "                for word in words.strip(\"\\n\").split(\" \"):\n",
    "                    word_frequency[word] += 1\n",
    "            self.word2id[\"<unk>\"] = 0\n",
    "            self.word2id[\"</s>\"] = 1\n",
    "            self.word2id[\"\"] = -1 #for padding\n",
    "            self.id2word[0] = \"<unk>\"\n",
    "            self.id2word[1] = \"</s>\"\n",
    "            self.id2word[-1] = \"\" #for padding\n",
    "            for i, (word, frequency) in zip(range(vocabulary_size - 2), sorted(sorted(word_frequency.items(), key = lambda x: x[0]), key = lambda x: x[1], reverse = True)):\n",
    "                self.word2id[word] = i + 2\n",
    "                self.id2word[i + 2] = word\n",
    "        self.size = len(self.word2id) - 1\n",
    "        return self\n",
    "\n",
    "    def save(self, path):\n",
    "        with open(path, \"w\") as f:\n",
    "            for i in range(self.size):\n",
    "                f.write(self.id2word[i] + \"\\n\")\n",
    "\n",
    "    def load(path):\n",
    "        self = Vocabulary()\n",
    "        self.word2id = defaultdict(lambda: 0)\n",
    "        self.id2word = dict()\n",
    "        i = 0\n",
    "        with open(path, \"w+\") as f:\n",
    "            for i, word in enumerate(f):\n",
    "                word = word.strip(\"\\n\")\n",
    "                self.word2id[word] = i\n",
    "                self.id2word[i] = word\n",
    "        self.size = i + 1\n",
    "        self.word2id[\"\"] = -1 #for padding\n",
    "        self.id2word[-1] = \"\" #for padding\n",
    "        return self\n",
    "\n",
    "def make_word2vec(path, embed_size):\n",
    "    word2vec_model = word2vec.Word2Vec(word2vec.LineSentence(path), size = embed_size, min_count = 1)\n",
    "    return word2vec_model\n",
    "\n",
    "def save_word2vec(word2vec_model, path):\n",
    "    word2vec_model.save(path)\n",
    "\n",
    "def load_word2vec(path):\n",
    "    return word2vec.Word2Vec.load(path)\n",
    "\n",
    "def convert_wordlist(batch, vocabulary):\n",
    "    for sentence in list(cuda.to_cpu(functions.transpose(functions.vstack(batch)).data)):\n",
    "        word_list = list()\n",
    "        for i in list(sentence):\n",
    "            word_list.append(vocabulary.id2word[i])\n",
    "        if \"</s>\" in word_list:\n",
    "            yield \" \".join(word_list[:word_list.index(\"</s>\")])\n",
    "        else:\n",
    "            yield \" \".join(word_list)\n",
    "\n",
    "def mono_batch(path, vocabulary, batch_size, lib):\n",
    "    with open(path, \"r\") as f:\n",
    "        batch = list()\n",
    "        for line in f:\n",
    "            wordid_list = list()\n",
    "            for word in line.strip(\"\\n\").split():\n",
    "                wordid_list.append(vocabulary.word2id[word])\n",
    "            wordid_list.append(vocabulary.word2id[\"</s>\"])\n",
    "            batch.append(wordid_list)\n",
    "            if len(batch) == batch_size:\n",
    "                yield [Variable(lib.array(list(x), dtype = lib.int32)) for x in zip_longest(*batch, fillvalue = -1)]\n",
    "                batch = list()\n",
    "        if len(batch) > 0:\n",
    "            yield [Variable(lib.array(list(x), dtype = lib.int32)) for x in zip_longest(*batch, fillvalue = -1)]\n",
    "\n",
    "def random_sorted_parallel_batch(source_path, target_path, source_vocabulary, target_vocabulary, batch_size, pooling, lib):\n",
    "    batch_list = list()\n",
    "    batch = list()\n",
    "    for n_pairs in generate_n_pairs(source_path, target_path, source_vocabulary, target_vocabulary, batch_size * pooling):\n",
    "        for st_pair in sorted(n_pairs, key = lambda x: len(x[0]), reverse = True):\n",
    "            batch.append(st_pair)\n",
    "            if len(batch) == batch_size:\n",
    "                batch_list.append(batch)\n",
    "                batch = list()\n",
    "    if len(batch) > 0:\n",
    "        batch_list.append(batch)\n",
    "    random.shuffle(batch_list)\n",
    "    for batch in batch_list:\n",
    "        batch_source = [batch[i][0] for i in range(len(batch))]\n",
    "        batch_target = [batch[i][1] for i in range(len(batch))]\n",
    "        yield ([Variable(lib.array(list(x), dtype = lib.int32)) for x in zip_longest(*batch_source, fillvalue = -1)], [Variable(lib.array(list(y), dtype = lib.int32)) for y in zip_longest(*batch_target, fillvalue = -1)])\n",
    "\n",
    "def random_sorted_3parallel_batch(source_path, target_path, output_path, source_vocabulary, target_vocabulary, batch_size, pooling, lib):\n",
    "    batch_list = list()\n",
    "    batch = list()\n",
    "    for n_pairs in generate_n_3pairs(source_path, target_path, output_path, source_vocabulary, target_vocabulary, batch_size * pooling):\n",
    "        for sto_pair in sorted(n_pairs, key = lambda x: len(x[0]), reverse = True):\n",
    "            batch.append(sto_pair)\n",
    "            if len(batch) == batch_size:\n",
    "                batch_list.append(batch)\n",
    "                batch = list()\n",
    "    if len(batch) > 0:\n",
    "        batch_list.append(batch)\n",
    "    random.shuffle(batch_list)\n",
    "    for batch in batch_list:\n",
    "        batch_source = [batch[i][0] for i in range(len(batch))]\n",
    "        batch_target = [batch[i][1] for i in range(len(batch))]\n",
    "        batch_output = [batch[i][2] for i in range(len(batch))]\n",
    "        yield ([Variable(lib.array(list(x), dtype = lib.int32)) for x in zip_longest(*batch_source, fillvalue = -1)], [Variable(lib.array(list(y), dtype = lib.int32)) for y in zip_longest(*batch_target, fillvalue = -1)], [Variable(lib.array(list(z), dtype = lib.int32)) for z in zip_longest(*batch_output, fillvalue = -1)])\n",
    "\n",
    "def generate_n_pairs(source_path, target_path, source_vocabulary, target_vocabulary, n):\n",
    "    with open(source_path, \"r\") as fs, open(target_path, \"r\") as ft:\n",
    "        n_pairs = list()\n",
    "        for line_source, line_target in zip(fs, ft):\n",
    "            wordid_source = list()\n",
    "            wordid_target = list()\n",
    "            for word in line_source.strip(\"\\n\").split():\n",
    "                wordid_source.append(source_vocabulary.word2id[word])\n",
    "            wordid_source.append(source_vocabulary.word2id[\"</s>\"])\n",
    "            for word in line_target.strip(\"\\n\").split():\n",
    "                wordid_target.append(target_vocabulary.word2id[word])\n",
    "            wordid_target.append(target_vocabulary.word2id[\"</s>\"])\n",
    "            n_pairs.append([wordid_source, wordid_target])\n",
    "            if len(n_pairs) == n:\n",
    "                yield n_pairs\n",
    "                n_pairs = list()\n",
    "        if len(n_pairs) > 0:\n",
    "            yield n_pairs\n",
    "\n",
    "def generate_n_3pairs(source_path, target_path, output_path, source_vocabulary, target_vocabulary, n):\n",
    "    with open(source_path, \"r\") as fs, open(target_path, \"r\") as ft, open(output_path, \"r\") as fo:\n",
    "        n_pairs = list()\n",
    "        for line_source, line_target, line_output in zip(fs, ft, fo):\n",
    "            wordid_source = list()\n",
    "            wordid_target = list()\n",
    "            wordid_output = list()\n",
    "            for word in line_source.strip(\"\\n\").split():\n",
    "                wordid_source.append(source_vocabulary.word2id[word])\n",
    "            wordid_source.append(source_vocabulary.word2id[\"</s>\"])\n",
    "            for word in line_target.strip(\"\\n\").split():\n",
    "                wordid_target.append(target_vocabulary.word2id[word])\n",
    "            wordid_target.append(target_vocabulary.word2id[\"</s>\"])\n",
    "            for word in line_output.strip(\"\\n\").split():\n",
    "                wordid_output.append(target_vocabulary.word2id[word])\n",
    "            wordid_output.append(target_vocabulary.word2id[\"</s>\"])\n",
    "            n_pairs.append([wordid_source, wordid_target, wordid_output])\n",
    "            if len(n_pairs) == n:\n",
    "                yield n_pairs\n",
    "                n_pairs = list()\n",
    "        if len(n_pairs) > 0:\n",
    "            yield n_pairs\n",
    "\n",
    "def copy_model(pre_model, model):\n",
    "    assert isinstance(pre_model, link.Chain)\n",
    "    assert isinstance(model, link.Chain)\n",
    "    for child in pre_model.children():\n",
    "        if child.name not in model.__dict__: continue\n",
    "        model_child = model[child.name]\n",
    "        if type(child) != type(model_child): continue\n",
    "        if isinstance(child, link.Chain):\n",
    "            copy_model(child, model_child)\n",
    "        if isinstance(child, link.Link):\n",
    "            match = True\n",
    "            for p, m in zip(child.namedparams(), model_child.namedparams()):\n",
    "                if p[0] != m[0]:\n",
    "                    match = False\n",
    "                    break\n",
    "                if p[1].data.shape != m[1].data.shape:\n",
    "                    match = False\n",
    "                    break\n",
    "            if not match:\n",
    "                continue\n",
    "            for p, m in zip(child.namedparams(), model_child.namedparams()):\n",
    "                p[1].data = m[1].data\n",
    "\n",
    "def trace(*args):\n",
    "\tprint(datetime.datetime.now(), '...', *args, file=sys.stderr)\n",
    "\tsys.stderr.flush()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
